[
  {
    "objectID": "Homework5.html",
    "href": "Homework5.html",
    "title": "HW5",
    "section": "",
    "text": "We use cross-validation and split the data into training and test sets to make sure that the model that we fit will work well on future datasets.\n\n\n\n\nCreate a bootstrap sample that has the same size (n) as the actual sample\nTrain the tree on this bootstrap sample, i.e. get 1 predicted y value using given x values in this sample.\nRepeat this process B = 1000 times, so you will have B predicted values, each from samples of size n.\nThen the final prediction is the average of the B predicted y values.\n\n\n\n\nA general linear model takes continuous response values, and it can have continuous and categorical predictors.\n\n\n\nAn interaction term captures the combined effect of the interacting predictors, so it can help model that one predictor may affect the response in different ways, depending on the level of another predictor. It allows the model to not assume that the effects of each predictor on the response are independent of the levels of another predictor.\n\n\n\nWe use the training data to fit an adequate model, and then we use the test data as “unseen” data to make sure that the model performs well."
  },
  {
    "objectID": "Homework5.html#what-is-the-purpose-of-using-cross-validation-when-fitting-a-random-forest-model",
    "href": "Homework5.html#what-is-the-purpose-of-using-cross-validation-when-fitting-a-random-forest-model",
    "title": "HW5",
    "section": "",
    "text": "We use cross-validation and split the data into training and test sets to make sure that the model that we fit will work well on future datasets."
  },
  {
    "objectID": "Homework5.html#describe-the-bagged-tree-algorithm.",
    "href": "Homework5.html#describe-the-bagged-tree-algorithm.",
    "title": "HW5",
    "section": "",
    "text": "Create a bootstrap sample that has the same size (n) as the actual sample\nTrain the tree on this bootstrap sample, i.e. get 1 predicted y value using given x values in this sample.\nRepeat this process B = 1000 times, so you will have B predicted values, each from samples of size n.\nThen the final prediction is the average of the B predicted y values."
  },
  {
    "objectID": "Homework5.html#what-is-meant-by-a-general-linear-model",
    "href": "Homework5.html#what-is-meant-by-a-general-linear-model",
    "title": "HW5",
    "section": "",
    "text": "A general linear model takes continuous response values, and it can have continuous and categorical predictors."
  },
  {
    "objectID": "Homework5.html#when-fitting-a-multiple-linear-regression-model-what-does-adding-an-interaction-term-do-that-is-what-does-it-allow-the-model-to-do-differently-as-compared-to-when-it-is-not-included-in-the-model",
    "href": "Homework5.html#when-fitting-a-multiple-linear-regression-model-what-does-adding-an-interaction-term-do-that-is-what-does-it-allow-the-model-to-do-differently-as-compared-to-when-it-is-not-included-in-the-model",
    "title": "HW5",
    "section": "",
    "text": "An interaction term captures the combined effect of the interacting predictors, so it can help model that one predictor may affect the response in different ways, depending on the level of another predictor. It allows the model to not assume that the effects of each predictor on the response are independent of the levels of another predictor."
  },
  {
    "objectID": "Homework5.html#why-do-we-split-our-data-into-a-training-and-test-set",
    "href": "Homework5.html#why-do-we-split-our-data-into-a-training-and-test-set",
    "title": "HW5",
    "section": "",
    "text": "We use the training data to fit an adequate model, and then we use the test data as “unseen” data to make sure that the model performs well."
  },
  {
    "objectID": "Homework5.html#quick-edadata-preparation",
    "href": "Homework5.html#quick-edadata-preparation",
    "title": "HW5",
    "section": "Quick EDA/Data Preparation",
    "text": "Quick EDA/Data Preparation\n\nQuickly understand your data. Check on missingness and summarize the data, especially with respect to the relationships of the variables to HeartDisease.\n\n\nheartData &lt;- read.csv(\"heart.csv\")\nmissingVars &lt;- colSums(is.na(heartData))\nprint(missingVars)\n\n           Age            Sex  ChestPainType      RestingBP    Cholesterol \n             0              0              0              0              0 \n     FastingBS     RestingECG          MaxHR ExerciseAngina        Oldpeak \n             0              0              0              0              0 \n      ST_Slope   HeartDisease \n             0              0 \n\n\nWe can see here that no data is missing for any of the variables."
  },
  {
    "objectID": "Homework5.html#split-your-data",
    "href": "Homework5.html#split-your-data",
    "title": "HW5",
    "section": "Split your Data",
    "text": "Split your Data\nHere, I subset to only numeric data for the kNN modeling that I will do next. Then I split this subset into training and test data\n\nset.seed(50)\nonlyNumericVars &lt;- newHeartData |&gt;\n  select(where(is.numeric), haveHeartDisease)\n\nonlyNumericVars$haveHeartDisease &lt;- droplevels(onlyNumericVars$haveHeartDisease)\n\ndiseaseIndex &lt;- createDataPartition(onlyNumericVars$haveHeartDisease, p = 0.8, list = FALSE)\ntrainData &lt;- onlyNumericVars[diseaseIndex, ]\ntestData &lt;- onlyNumericVars[-diseaseIndex, ]"
  },
  {
    "objectID": "Homework5.html#knn",
    "href": "Homework5.html#knn",
    "title": "HW5",
    "section": "kNN",
    "text": "kNN\nFit a kNN model using all the numeric variables as predictors and then used 10-fold cross validation, with 3 repeats. The tuning parameter k can take values 1,2,…,40\n\ntrctrl &lt;- trainControl(method = \"repeatedcv\", number = 10, repeats = 3)\nknn_fit &lt;- train(haveHeartDisease ~ ., data = trainData, \n                 method = \"knn\",\n                 trControl=trctrl,\n                 preProcess = c(\"center\", \"scale\"),\n                 tuneGrid = expand.grid(k = 1:40))\n\nChecking the accuracy with confusionMatrix()\n\nconfusionMatrix(data=testData$haveHeartDisease, reference = predict(knn_fit, newdata = testData))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 60 22\n         1 18 83\n                                         \n               Accuracy : 0.7814         \n                 95% CI : (0.7145, 0.839)\n    No Information Rate : 0.5738         \n    P-Value [Acc &gt; NIR] : 2.949e-09      \n                                         \n                  Kappa : 0.556          \n                                         \n Mcnemar's Test P-Value : 0.6353         \n                                         \n            Sensitivity : 0.7692         \n            Specificity : 0.7905         \n         Pos Pred Value : 0.7317         \n         Neg Pred Value : 0.8218         \n             Prevalence : 0.4262         \n         Detection Rate : 0.3279         \n   Detection Prevalence : 0.4481         \n      Balanced Accuracy : 0.7799         \n                                         \n       'Positive' Class : 0"
  },
  {
    "objectID": "Homework5.html#logistic-regression",
    "href": "Homework5.html#logistic-regression",
    "title": "HW5",
    "section": "Logistic Regression",
    "text": "Logistic Regression\nFirst I split the data into training and test sets, this time with all of the predictors present in the datasets.\n\nset.seed(50)\nnewHeartData$haveHeartDisease &lt;- droplevels(newHeartData$haveHeartDisease)\n\ndiseaseIndexLog &lt;- createDataPartition(newHeartData$haveHeartDisease, p = 0.8, list = FALSE)\ntrainDataLog &lt;- newHeartData[diseaseIndexLog, ]\ntestDataLog &lt;- newHeartData[-diseaseIndexLog, ]\n\nModel 1:\n\nlogFit1 &lt;- train(haveHeartDisease ~ Age + Sex + Cholesterol, data = trainDataLog, \n                 method = \"glm\",\n                 family = \"binomial\",\n                 trControl=trctrl,\n                 preProcess = c(\"center\", \"scale\"))\nlogFit1\n\nGeneralized Linear Model \n\n735 samples\n  3 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (3), scaled (3) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 661, 661, 662, 661, 662, 661, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.6815458  0.3475213\n\n\nModel 2:\n\nlogFit2 &lt;- train(haveHeartDisease ~ Age + RestingBP + MaxHR + Cholesterol, data = trainDataLog, \n                 method = \"glm\",\n                 family = \"binomial\",\n                 trControl=trctrl,\n                 preProcess = c(\"center\", \"scale\"))\nlogFit2\n\nGeneralized Linear Model \n\n735 samples\n  4 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (4), scaled (4) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 662, 661, 661, 662, 661, 661, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.6993535  0.3880177\n\n\nModel 3:\n\nlogFit3 &lt;- train(haveHeartDisease ~ Age + RestingBP + MaxHR + Cholesterol + FastingBS, data = trainDataLog, \n                 method = \"glm\",\n                 family = \"binomial\",\n                 trControl=trctrl,\n                 preProcess = c(\"center\", \"scale\"))\nlogFit3\n\nGeneralized Linear Model \n\n735 samples\n  5 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (5), scaled (5) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 661, 661, 662, 662, 661, 663, ... \nResampling results:\n\n  Accuracy   Kappa    \n  0.7016638  0.3940734\n\nsummary(logFit3)\n\n\nCall:\nNULL\n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  0.31491    0.08720   3.611 0.000304 ***\nAge          0.32330    0.09675   3.342 0.000833 ***\nRestingBP    0.12929    0.09184   1.408 0.159192    \nMaxHR       -0.71618    0.10046  -7.129 1.01e-12 ***\nCholesterol -0.29863    0.09747  -3.064 0.002184 ** \nFastingBS    0.52178    0.09846   5.299 1.16e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 1010.42  on 734  degrees of freedom\nResidual deviance:  815.53  on 729  degrees of freedom\nAIC: 827.53\n\nNumber of Fisher Scoring iterations: 4\n\n\nCheck Accuracy of Model 3:\n\nconfusionMatrix(data=testDataLog$haveHeartDisease, reference = predict(logFit3, newdata = testDataLog))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 53 29\n         1 29 72\n                                          \n               Accuracy : 0.6831          \n                 95% CI : (0.6103, 0.7497)\n    No Information Rate : 0.5519          \n    P-Value [Acc &gt; NIR] : 0.0001985       \n                                          \n                  Kappa : 0.3592          \n                                          \n Mcnemar's Test P-Value : 1.0000000       \n                                          \n            Sensitivity : 0.6463          \n            Specificity : 0.7129          \n         Pos Pred Value : 0.6463          \n         Neg Pred Value : 0.7129          \n             Prevalence : 0.4481          \n         Detection Rate : 0.2896          \n   Detection Prevalence : 0.4481          \n      Balanced Accuracy : 0.6796          \n                                          \n       'Positive' Class : 0"
  },
  {
    "objectID": "Homework5.html#tree-models",
    "href": "Homework5.html#tree-models",
    "title": "HW5",
    "section": "Tree Models",
    "text": "Tree Models\nClassification Tree Model\n\nclassTree &lt;- train(haveHeartDisease ~ Age + RestingBP + MaxHR + Cholesterol + FastingBS, data = trainDataLog, \n                 method = \"rpart\",\n                 trControl=trctrl,\n                 preProcess = c(\"center\", \"scale\"),\n                 tuneGrid = expand.grid(cp = seq(0,0.1, by=0.001)))\nclassTree\n\nCART \n\n735 samples\n  5 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (5), scaled (5) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 661, 661, 661, 662, 661, 662, ... \nResampling results across tuning parameters:\n\n  cp     Accuracy   Kappa    \n  0.000  0.6911963  0.3714969\n  0.001  0.6930043  0.3756244\n  0.002  0.7006928  0.3921221\n  0.003  0.7011558  0.3932201\n  0.004  0.7056603  0.4029693\n  0.005  0.7047906  0.3986353\n  0.006  0.7029394  0.3921540\n  0.007  0.7038209  0.3926001\n  0.008  0.7069803  0.3983200\n  0.009  0.7088008  0.4015201\n  0.010  0.7106149  0.4046887\n  0.011  0.7092886  0.4020647\n  0.012  0.7133611  0.4104690\n  0.013  0.7133611  0.4104690\n  0.014  0.7120160  0.4074533\n  0.015  0.7120160  0.4074533\n  0.016  0.7147372  0.4134011\n  0.017  0.7188097  0.4206687\n  0.018  0.7188097  0.4206687\n  0.019  0.7188097  0.4206687\n  0.020  0.7188097  0.4206687\n  0.021  0.7178526  0.4187931\n  0.022  0.7178526  0.4187931\n  0.023  0.7178526  0.4187931\n  0.024  0.7201107  0.4227704\n  0.025  0.7201107  0.4227704\n  0.026  0.7201107  0.4227704\n  0.027  0.7201107  0.4227704\n  0.028  0.7164577  0.4179928\n  0.029  0.7164577  0.4179928\n  0.030  0.7164577  0.4179928\n  0.031  0.7110338  0.4089422\n  0.032  0.7110338  0.4089422\n  0.033  0.7110338  0.4089422\n  0.034  0.7087816  0.4048523\n  0.035  0.7087816  0.4048523\n  0.036  0.7087816  0.4048523\n  0.037  0.7087816  0.4048523\n  0.038  0.7074364  0.4052767\n  0.039  0.7074364  0.4052767\n  0.040  0.7074364  0.4052767\n  0.041  0.7047402  0.4024716\n  0.042  0.7047402  0.4024716\n  0.043  0.7047402  0.4024716\n  0.044  0.7038269  0.4007371\n  0.045  0.7024756  0.3982897\n  0.046  0.7024756  0.3982897\n  0.047  0.7024756  0.3982897\n  0.048  0.7020128  0.3988437\n  0.049  0.7020128  0.3988437\n  0.050  0.7020128  0.3988437\n  0.051  0.7015624  0.3983913\n  0.052  0.7015624  0.3983913\n  0.053  0.7015624  0.3983913\n  0.054  0.7015624  0.3983913\n  0.055  0.7015624  0.3983913\n  0.056  0.7015624  0.3983913\n  0.057  0.7015624  0.3983913\n  0.058  0.7015624  0.3983913\n  0.059  0.7015624  0.3983913\n  0.060  0.7015624  0.3983913\n  0.061  0.7015624  0.3983913\n  0.062  0.7015624  0.3983913\n  0.063  0.7015624  0.3983913\n  0.064  0.7015624  0.3983913\n  0.065  0.7015624  0.3983913\n  0.066  0.7015624  0.3983913\n  0.067  0.7015624  0.3983913\n  0.068  0.7015624  0.3983913\n  0.069  0.7015624  0.3983913\n  0.070  0.7015624  0.3983913\n  0.071  0.7015624  0.3983913\n  0.072  0.7015624  0.3983913\n  0.073  0.7015624  0.3983913\n  0.074  0.7015624  0.3983913\n  0.075  0.7006614  0.3978902\n  0.076  0.7006614  0.3978902\n  0.077  0.7006614  0.3978902\n  0.078  0.6997605  0.3970054\n  0.079  0.6997605  0.3970054\n  0.080  0.6997605  0.3970054\n  0.081  0.6997605  0.3970054\n  0.082  0.6969953  0.3931329\n  0.083  0.6969953  0.3931329\n  0.084  0.6969953  0.3931329\n  0.085  0.6901953  0.3815064\n  0.086  0.6901953  0.3815064\n  0.087  0.6901953  0.3815064\n  0.088  0.6883435  0.3782751\n  0.089  0.6860912  0.3746477\n  0.090  0.6860912  0.3746477\n  0.091  0.6860912  0.3746477\n  0.092  0.6811178  0.3654957\n  0.093  0.6811178  0.3654957\n  0.094  0.6811178  0.3654957\n  0.095  0.6783777  0.3611653\n  0.096  0.6783777  0.3611653\n  0.097  0.6783777  0.3611653\n  0.098  0.6779211  0.3604471\n  0.099  0.6761131  0.3582370\n  0.100  0.6761131  0.3582370\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was cp = 0.027.\n\n\nRandom Forest Model\n\nrandForest &lt;- train(haveHeartDisease ~ Age + RestingBP + MaxHR + Cholesterol + FastingBS, data = trainDataLog, \n                 method = \"rf\",\n                 trControl=trctrl,\n                 preProcess = c(\"center\", \"scale\"),\n                 tuneGrid = data.frame(mtry = 1:5))\nrandForest\n\nRandom Forest \n\n735 samples\n  5 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (5), scaled (5) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 661, 662, 661, 661, 662, 662, ... \nResampling results across tuning parameters:\n\n  mtry  Accuracy   Kappa    \n  1     0.7155467  0.4208435\n  2     0.7142447  0.4196833\n  3     0.7101658  0.4124980\n  4     0.7119923  0.4161684\n  5     0.7047296  0.4010813\n\nAccuracy was used to select the optimal model using the largest value.\nThe final value used for the model was mtry = 1.\n\n\nBoosted Tree Model\n\nboosted &lt;- train(haveHeartDisease ~ Age + RestingBP + MaxHR + Cholesterol + FastingBS, data = trainDataLog, \n                 method = \"gbm\",\n                 trControl=trctrl,\n                 preProcess = c(\"center\", \"scale\"),\n                 tuneGrid = expand.grid(n.trees = c(25,50,100,200),\n                                        interaction.depth = c(1,2,3),\n                                        shrinkage = 0.1,\n                                        n.minobsinnode = 10),\n                 verbose = FALSE)\nboosted\n\nStochastic Gradient Boosting \n\n735 samples\n  5 predictor\n  2 classes: '0', '1' \n\nPre-processing: centered (5), scaled (5) \nResampling: Cross-Validated (10 fold, repeated 3 times) \nSummary of sample sizes: 661, 662, 661, 661, 662, 661, ... \nResampling results across tuning parameters:\n\n  interaction.depth  n.trees  Accuracy   Kappa    \n  1                   25      0.7220536  0.4361956\n  1                   50      0.7174812  0.4263257\n  1                  100      0.7197579  0.4302200\n  1                  200      0.7107368  0.4133044\n  2                   25      0.7261071  0.4419706\n  2                   50      0.7211275  0.4322226\n  2                  100      0.7183944  0.4267090\n  2                  200      0.7161727  0.4240390\n  3                   25      0.7238679  0.4357722\n  3                   50      0.7179495  0.4247680\n  3                  100      0.7157164  0.4211718\n  3                  200      0.7043931  0.3993965\n\nTuning parameter 'shrinkage' was held constant at a value of 0.1\n\nTuning parameter 'n.minobsinnode' was held constant at a value of 10\nAccuracy was used to select the optimal model using the largest value.\nThe final values used for the model were n.trees = 25, interaction.depth =\n 2, shrinkage = 0.1 and n.minobsinnode = 10.\n\n\n\nconfusionMatrix(data=testDataLog$haveHeartDisease, reference = predict(classTree, newdata = testDataLog))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 45 37\n         1 18 83\n                                          \n               Accuracy : 0.6995          \n                 95% CI : (0.6274, 0.7649)\n    No Information Rate : 0.6557          \n    P-Value [Acc &gt; NIR] : 0.12096         \n                                          \n                  Kappa : 0.3788          \n                                          \n Mcnemar's Test P-Value : 0.01522         \n                                          \n            Sensitivity : 0.7143          \n            Specificity : 0.6917          \n         Pos Pred Value : 0.5488          \n         Neg Pred Value : 0.8218          \n             Prevalence : 0.3443          \n         Detection Rate : 0.2459          \n   Detection Prevalence : 0.4481          \n      Balanced Accuracy : 0.7030          \n                                          \n       'Positive' Class : 0               \n                                          \n\nconfusionMatrix(data=testDataLog$haveHeartDisease, reference = predict(randForest, newdata = testDataLog))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 53 29\n         1 23 78\n                                          \n               Accuracy : 0.7158          \n                 95% CI : (0.6446, 0.7799)\n    No Information Rate : 0.5847          \n    P-Value [Acc &gt; NIR] : 0.0001613       \n                                          \n                  Kappa : 0.4215          \n                                          \n Mcnemar's Test P-Value : 0.4880741       \n                                          \n            Sensitivity : 0.6974          \n            Specificity : 0.7290          \n         Pos Pred Value : 0.6463          \n         Neg Pred Value : 0.7723          \n             Prevalence : 0.4153          \n         Detection Rate : 0.2896          \n   Detection Prevalence : 0.4481          \n      Balanced Accuracy : 0.7132          \n                                          \n       'Positive' Class : 0               \n                                          \n\nconfusionMatrix(data=testDataLog$haveHeartDisease, reference = predict(boosted, newdata = testDataLog))\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction  0  1\n         0 50 32\n         1 22 79\n                                          \n               Accuracy : 0.7049          \n                 95% CI : (0.6332, 0.7699)\n    No Information Rate : 0.6066          \n    P-Value [Acc &gt; NIR] : 0.003574        \n                                          \n                  Kappa : 0.3965          \n                                          \n Mcnemar's Test P-Value : 0.220671        \n                                          \n            Sensitivity : 0.6944          \n            Specificity : 0.7117          \n         Pos Pred Value : 0.6098          \n         Neg Pred Value : 0.7822          \n             Prevalence : 0.3934          \n         Detection Rate : 0.2732          \n   Detection Prevalence : 0.4481          \n      Balanced Accuracy : 0.7031          \n                                          \n       'Positive' Class : 0"
  },
  {
    "objectID": "Homework5.html#wrap-up",
    "href": "Homework5.html#wrap-up",
    "title": "HW5",
    "section": "Wrap up",
    "text": "Wrap up\nThe random forest model had the highest accuracy at 71.6\\(\\%\\), based on the different confusionMatrix() outputs."
  }
]